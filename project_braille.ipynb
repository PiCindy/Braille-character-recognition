{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braille detection with CNN\n",
    "\n",
    "In this script we create, train and evaluate an image classifier based on convolutional networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fnmatch\n",
    "from skimage import io, transform\n",
    "import torchvision\n",
    "\n",
    "# Define the device and data repository\n",
    "device = 'cpu'\n",
    "data_dir = 'archive/Braille Dataset/Braille Dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data management\n",
    "\n",
    "We read data, split it onto train, val and test and create data loaders with preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a class object for representing our image data\n",
    "# This is a subclass of torch.utils.data.dataset.Dataset that will serve as input to the DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        \"\"\"Here we initialize the attributes of the object of the class.\"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = sorted(self._find_files(image_dir))\n",
    "        self.classes = [filename.split(\"/\")[-1][0] for filename in self.image_files]\n",
    "        self.let2num = {a: i for i, a in enumerate(sorted(list(set(self.classes))))}\n",
    "        self.num2let = {i: a for a, i in self.let2num.items()}\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def letter_to_number(self, let):\n",
    "        return self.let2num[let]\n",
    "    \n",
    "    def number_to_letter(self, num):\n",
    "        return self.num2let[num]\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Here we return the size of the dataset.\"\"\"\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Here we return a data sample for a given index.\"\"\"\n",
    "        # TO DO: write this function: it takes as input 'index' (which is an integer number),\n",
    "        # and returns the corresponding item as a pytorch tensor\n",
    "        # hint: use 'self.image_files' , which is the list of images paths defined in the '__init__' method\n",
    "        filename = self.image_files[index]\n",
    "        x = io.imread(filename)\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, self.classes[index]\n",
    "\n",
    "    def _find_files(self, directory, pattern='*.jpg'):\n",
    "        \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "        files = []\n",
    "        for root, dirnames, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                files.append(os.path.join(root, filename))\n",
    "        return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to divide: torch.utils.data.random_split(dataset, lengths)\n",
    "data_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize(\n",
    "                                                      mean=(0.5, 0.5, 0.5), \n",
    "                                                      std=(0.3,0.3,0.3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(image_dir=data_dir, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd0f9115430>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[0][0].numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 28])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.letter_to_number(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.number_to_letter(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ln = len(dataset)\n",
    "train_ln, val_ln, test_ln = int(dataset_ln * 0.8), int(dataset_ln * 0.1), int(dataset_ln * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = torch.utils.data.random_split(dataset, [train_ln, val_ln, test_ln], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          ...,\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536]],\n",
       " \n",
       "         [[1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          ...,\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536]],\n",
       " \n",
       "         [[1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          ...,\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536]]]),\n",
       " 'w')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    images, labels = zip(*data)\n",
    "    \n",
    "    labels = [dataset.letter_to_number(label) for label in labels]\n",
    "    return torch.stack(images).float(), torch.tensor(labels).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Here we create several different models to compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN classifier module.\n",
    "\n",
    "class CNNSimpleClassif(nn.Module):\n",
    "    def __init__(self, num_channels1=16, num_channels2=32, num_classes=26):\n",
    "        super(CNNSimpleClassif, self).__init__()\n",
    "        \n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(3, num_channels1, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm2d(16),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        self.l2 = nn.Sequential(nn.Conv2d(num_channels1, num_channels2, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm2d(32),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        size_vec = 1568\n",
    "        self.res = nn.Linear(size_vec, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.l2(self.l1(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = self.res(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, eval_dataloader, device='cpu'):\n",
    "\n",
    "    # Copy the model to the device\n",
    "    model.to(device)\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        \n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "\n",
    "            # Get the predicted labels classes\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_predicted = model(images)\n",
    "            \n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            \n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, device='cpu', verbose=True, model_name='simple'):\n",
    "\n",
    "    # Copy the model to the device and set it in 'training' mode (thus all gradients are computed)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list to record the training loss over epochs\n",
    "    loss_all_epochs = []\n",
    "    \n",
    "    last_best_acc = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        loss_current_epoch = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            #print(labels)\n",
    "            #print(images.size(), labels.size(), labels)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            y_pred = model(images)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_current_epoch += loss.item()\n",
    "            \n",
    "        cur_acc = eval_model(model, valid_dataloader)\n",
    "        if cur_acc > last_best_acc:\n",
    "            last_best_acc = cur_acc\n",
    "            torch.save(model.state_dict(), f'model_{model_name}_best.pt')\n",
    "            print(\"\\nNew best accuracy! \", last_best_acc)\n",
    "            \n",
    "        # At the end of each epoch, record and display the loss over all batches\n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        if verbose:\n",
    "            print('\\rEpoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch), end='')\n",
    "        \n",
    "    return model, loss_all_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels1 = 16\n",
    "num_channels2 = 32\n",
    "num_classes = 26\n",
    "model = CNNSimpleClassif(num_channels1, num_channels2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best accuracy!  40.38461538461539\n",
      "Epoch [1/20], Loss: 441.1668\n",
      "New best accuracy!  55.12820512820513\n",
      "Epoch [2/20], Loss: 260.3588\n",
      "New best accuracy!  56.41025641025641\n",
      "Epoch [3/20], Loss: 183.4120\n",
      "New best accuracy!  62.17948717948718\n",
      "Epoch [4/20], Loss: 143.7706\n",
      "New best accuracy!  69.23076923076923\n",
      "Epoch [6/20], Loss: 92.86337\n",
      "New best accuracy!  73.07692307692308\n",
      "Epoch [7/20], Loss: 76.1755\n",
      "New best accuracy!  73.71794871794872\n",
      "Epoch [8/20], Loss: 60.7010\n",
      "New best accuracy!  77.56410256410257\n",
      "Epoch [13/20], Loss: 18.8131\n",
      "New best accuracy!  82.05128205128206\n",
      "Epoch [20/20], Loss: 3.60545"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimizer\n",
    "num_epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training\n",
    "model, loss_total = train_model(model, train_dataloader, val_dataloader, num_epochs, loss_fn, learning_rate, device='cpu', verbose=True)\n",
    "\n",
    "# Save the model and display the loss over epochs\n",
    "torch.save(model.state_dict(), 'model_simple.pt')\n",
    "plt.plot(loss_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.6923076923077\n"
     ]
    }
   ],
   "source": [
    "model = CNNSimpleClassif(num_channels1, num_channels2, num_classes)\n",
    "\n",
    "model.load_state_dict(torch.load('model_simple_best.pt'))\n",
    "\n",
    "res = eval_model(model, test_dataloader, device='cpu')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
