{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braille detection with CNN\n",
    "### Justine Diliberto, Anna Nikiforovskaja, Cindy Pereira\n",
    "\n",
    "In this script we create, train and evaluate an image classifier based on convolutional networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fnmatch\n",
    "from skimage import io, transform\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "\n",
    "# Define the device and data repository\n",
    "device = 'cpu'\n",
    "data_dir = 'dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data management\n",
    "\n",
    "We read data, split it onto train, val and test and create data loaders with preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a class object for representing our image data\n",
    "# This is a subclass of torch.utils.data.dataset.Dataset that will serve as input to the DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        \"\"\"Initialize the attributes of the object of the class.\"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = sorted(self._find_files(image_dir))\n",
    "        # Get the first character in each filename: corresponds to the letter of the file\n",
    "        # ex: dataset/a1.JPG0dim.jpg -> get the \"a\"\n",
    "        self.classes = [filename.split(\"/\")[-1][0] for filename in self.image_files]\n",
    "        # Create a dictionary associating a number to each letter\n",
    "        self.let2num = {a: i for i, a in enumerate(sorted(list(set(self.classes))))}\n",
    "        # Create the reverted dictionary with number as key\n",
    "        self.num2let = {i: a for a, i in self.let2num.items()}\n",
    "        self.transform = transform\n",
    "        \n",
    "    def letter_to_number(self, let):\n",
    "        \"\"\"Get the number associated to a letter\"\"\"\n",
    "        return self.let2num[let]\n",
    "    \n",
    "    def number_to_letter(self, num):\n",
    "        \"\"\"Get the letter associated to a number\"\"\"\n",
    "        return self.num2let[num]\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the size of the dataset.\"\"\"\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Here we return a data sample for a given index.\"\"\"\n",
    "        filename = self.image_files[index]\n",
    "        x = io.imread(filename)\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, self.classes[index]\n",
    "\n",
    "    def _find_files(self, directory, pattern='*.jpg'):\n",
    "        \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "        files = []\n",
    "        for root, dirnames, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                files.append(os.path.join(root, filename))\n",
    "        return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform to apply on the data\n",
    "data_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize(\n",
    "                                                      mean=(0.5, 0.5, 0.5), \n",
    "                                                      std=(0.3,0.3,0.3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = ImageDataset(image_dir=data_dir, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that our Dataset class and functions work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f01147fcd60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[0][0].numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.letter_to_number(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.number_to_letter(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lengths of train (80%), validation (10%) and test (10%) sets\n",
    "dataset_ln = len(dataset)\n",
    "train_ln, val_ln, test_ln = int(dataset_ln * 0.8), int(dataset_ln * 0.1), int(dataset_ln * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation and test sets\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(dataset, [train_ln, val_ln, test_ln], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          ...,\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536]],\n",
       " \n",
       "         [[1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          ...,\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536]],\n",
       " \n",
       "         [[1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          [1.6667, 1.6667, 1.6536,  ..., 1.6013, 1.6275, 1.6536],\n",
       "          ...,\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536],\n",
       "          [1.6536, 1.6536, 1.6536,  ..., 1.6405, 1.6405, 1.6536]]]),\n",
       " 'w')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"Associate images with labels\"\"\"\n",
    "    images, labels = zip(*data)\n",
    "    # Get the number corresponding to each label\n",
    "    labels = [dataset.letter_to_number(label) for label in labels]\n",
    "    return torch.stack(images).float(), torch.tensor(labels).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using a batch size of 8\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, \n",
    "                device='cpu', verbose=True, model_name='simple', optim='SGD', cells=False):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "\n",
    "    # Copy the model to the device and set it in 'training' mode (thus all gradients are computed)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    if optim == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Initialize a list to record the training loss over epochs\n",
    "    loss_all_epochs = []\n",
    "    \n",
    "    last_best_acc = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # Initialize the training loss for the current epoch\n",
    "        loss_current_epoch = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            #print(labels)\n",
    "            #print(images.size(), labels.size(), labels)\n",
    "            images = images.to(device)\n",
    "            # cells is True if we use the 3rd model\n",
    "            if cells:\n",
    "                # In this case, labels are bits where 1 corresponds to a black dot in Braille\n",
    "                new_labels = torch.zeros((len(labels), 6))\n",
    "                for i, l in enumerate(labels):\n",
    "                    new_labels[i] = torch.tensor(list(map(int, braille_map[dataset.number_to_letter(l.item())]))).float()\n",
    "                labels = new_labels\n",
    "            else:\n",
    "                labels = labels.to(device)\n",
    "            # Predict ys\n",
    "            y_pred = model(images)\n",
    "            # Apply loss function on predictions\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            \n",
    "            # Compute the gradient\n",
    "            loss.backward()\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            # Set the gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "            # Update the loss\n",
    "            loss_current_epoch += loss.item()\n",
    "        \n",
    "        # Compute the accuracy\n",
    "        cur_acc = eval_model(model, valid_dataloader)\n",
    "        # Save the parameters it's the new best accuracy \n",
    "        if cur_acc > last_best_acc:\n",
    "            last_best_acc = cur_acc\n",
    "            torch.save(model.state_dict(), f'models/model_{model_name}_best.pt')\n",
    "            print(\"\\nNew best accuracy! \", last_best_acc)\n",
    "            \n",
    "        # At the end of each epoch, record and display the loss over all batches\n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        if verbose:\n",
    "            print('\\rEpoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch), end='')\n",
    "        \n",
    "    return model, loss_all_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, eval_dataloader, device='cpu'):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    \n",
    "    # Copy the model to the device\n",
    "    model.to(device)\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # Initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        \n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "\n",
    "            # Get the predicted labels classes\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_predicted = model(images)\n",
    "            \n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            \n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the results\n",
    "all_results = {\"Model\":[], \"Loss\":[], \"Optimizer\":[], \"lr\":[], \"Accuracy\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN classifier module.\n",
    "\n",
    "class CNNSimpleClassif(nn.Module):\n",
    "    def __init__(self, num_channels1=16, num_channels2=32, num_classes=26):\n",
    "        \"\"\"Models a simple classifier\"\"\"\n",
    "        super(CNNSimpleClassif, self).__init__()\n",
    "        \n",
    "        # Create two sequential layers with convolution, ReLU, BatchNorm and Max Pooling\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(3, num_channels1, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm2d(16),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        self.l2 = nn.Sequential(nn.Conv2d(num_channels1, num_channels2, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm2d(32),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        size_vec = 1568\n",
    "        # Create a linear layer\n",
    "        self.res = nn.Linear(size_vec, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Move from the input layers to the output layer\"\"\"\n",
    "        x = self.l2(self.l1(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = self.res(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "num_channels1 = 16\n",
    "num_channels2 = 32\n",
    "num_classes = 26\n",
    "# Create the model following the simple classifier\n",
    "model = CNNSimpleClassif(num_channels1, num_channels2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best accuracy!  44.87179487179487\n",
      "Epoch [1/20], Loss: 424.5679\n",
      "New best accuracy!  58.97435897435897\n",
      "Epoch [2/20], Loss: 260.0883\n",
      "New best accuracy!  63.46153846153846\n",
      "Epoch [3/20], Loss: 201.2330\n",
      "New best accuracy!  66.66666666666667\n",
      "Epoch [4/20], Loss: 167.8934\n",
      "New best accuracy!  67.94871794871794\n",
      "Epoch [5/20], Loss: 144.9518\n",
      "New best accuracy!  72.43589743589743\n",
      "Epoch [9/20], Loss: 86.36466\n",
      "New best accuracy!  75.0\n",
      "Epoch [11/20], Loss: 66.9110\n",
      "New best accuracy!  76.28205128205128\n",
      "Epoch [12/20], Loss: 60.0730\n",
      "New best accuracy!  78.2051282051282\n",
      "Epoch [14/20], Loss: 47.9569\n",
      "New best accuracy!  78.84615384615384\n",
      "Epoch [15/20], Loss: 42.4421\n",
      "New best accuracy!  79.48717948717949\n",
      "Epoch [16/20], Loss: 38.0731\n",
      "New best accuracy!  80.12820512820512\n",
      "Epoch [17/20], Loss: 33.1960\n",
      "New best accuracy!  80.76923076923077\n",
      "Epoch [19/20], Loss: 26.4306\n",
      "New best accuracy!  81.41025641025641\n",
      "Epoch [20/20], Loss: 23.8893"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimizer\n",
    "num_epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training\n",
    "model, loss_total = train_model(model, train_dataloader, val_dataloader, num_epochs, loss_fn, learning_rate, device='cpu', verbose=True)\n",
    "\n",
    "# Save the model and display the loss over epochs\n",
    "torch.save(model.state_dict(), 'models/model_simple.pt')\n",
    "plt.plot(loss_total)\n",
    "\n",
    "# Plot the results and save the figure\n",
    "plt.title(\"Loss for the simple model\")\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"CrossEntropyLoss\")\n",
    "plt.savefig(\"plots/model_simple_loss.png\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.33333333333333\n"
     ]
    }
   ],
   "source": [
    "# Create the model using simple classifier\n",
    "model = CNNSimpleClassif(num_channels1, num_channels2, num_classes)\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('models/model_simple_best.pt'))\n",
    "# Compute the accuracy\n",
    "res = eval_model(model, test_dataloader, device='cpu')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill dataframe with the results of this model\n",
    "all_results[\"Model\"].append(\"simple\")\n",
    "all_results[\"Loss\"].append(\"CrossEntropyLoss\")\n",
    "all_results[\"Optimizer\"].append(\"SGD\")\n",
    "all_results[\"lr\"].append(learning_rate)\n",
    "all_results[\"Accuracy\"].append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN classifier module.\n",
    "\n",
    "class CNNSimple6Cells(nn.Module):\n",
    "    def __init__(self, num_channels1=16, num_channels2=32, num_classes=26):\n",
    "        \"\"\"Models a classifier using the 6 dots\"\"\"\n",
    "        super(CNNSimple6Cells, self).__init__()\n",
    "        \n",
    "        # Create two sequential layers with convolution, ReLU, BatchNorm and Max Pooling\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(3, num_channels1, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm2d(16),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        self.l2 = nn.Sequential(nn.Conv2d(num_channels1, num_channels2, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm2d(32),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        size_vec = 1568\n",
    "        # Create a sequential layer with linear and ReLU \n",
    "        self.lin6 = nn.Sequential(nn.Linear(size_vec, 6), nn.ReLU())\n",
    "        # Create a sequential layer with linear, ReLU and another linear\n",
    "        self.res = nn.Sequential(nn.Linear(6, 100),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(100, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Move from the input layers to the output layer\"\"\"\n",
    "        x = self.l2(self.l1(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = self.res(self.lin6(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best accuracy!  21.794871794871796\n",
      "Epoch [1/40], Loss: 449.3016\n",
      "New best accuracy!  33.333333333333336\n",
      "Epoch [2/40], Loss: 340.4751\n",
      "New best accuracy!  49.35897435897436\n",
      "Epoch [3/40], Loss: 271.5713\n",
      "New best accuracy!  56.41025641025641\n",
      "Epoch [4/40], Loss: 220.4931\n",
      "New best accuracy!  57.05128205128205\n",
      "Epoch [5/40], Loss: 191.6558\n",
      "New best accuracy!  62.82051282051282\n",
      "Epoch [7/40], Loss: 158.9903\n",
      "New best accuracy!  65.38461538461539\n",
      "Epoch [8/40], Loss: 142.7732\n",
      "New best accuracy!  68.58974358974359\n",
      "Epoch [9/40], Loss: 125.4904\n",
      "New best accuracy!  71.15384615384616\n",
      "Epoch [10/40], Loss: 113.6389\n",
      "New best accuracy!  74.35897435897436\n",
      "Epoch [13/40], Loss: 91.80490\n",
      "New best accuracy!  77.56410256410257\n",
      "Epoch [19/40], Loss: 52.8179\n",
      "New best accuracy!  78.84615384615384\n",
      "Epoch [20/40], Loss: 59.3241\n",
      "New best accuracy!  80.12820512820512\n",
      "Epoch [26/40], Loss: 39.8912"
     ]
    }
   ],
   "source": [
    "# Set the parameters\n",
    "num_channels1 = 16\n",
    "num_channels2 = 32\n",
    "num_classes = 26\n",
    "# Create the model\n",
    "model = CNNSimple6Cells(num_channels1, num_channels2, num_classes)\n",
    "\n",
    "# Optimizer\n",
    "num_epochs = 40\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.003\n",
    "\n",
    "# Training\n",
    "model, loss_total = train_model(model, train_dataloader, val_dataloader, num_epochs, loss_fn, learning_rate, device='cpu', verbose=True, model_name=\"6cells\", optim='Adam')\n",
    "\n",
    "# Save the model and display the loss over epochs\n",
    "torch.save(model.state_dict(), 'models/model_simple_6_cells.pt')\n",
    "\n",
    "# Plot the results and save the figure\n",
    "plt.plot(loss_total)\n",
    "plt.title(\"Loss for the 6cells model\")\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"CrossEntropyLoss\")\n",
    "plt.savefig(\"plots/model_simple_6cells_loss.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 6 cells model\n",
    "model = CNNSimple6Cells(num_channels1, num_channels2, num_classes)\n",
    "# Load the parameters\n",
    "model.load_state_dict(torch.load('models/model_6cells_best.pt'))\n",
    "# Compute the accuracy\n",
    "res = eval_model(model, test_dataloader, device='cpu')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill dataframe with the results of this model\n",
    "all_results[\"Model\"].append(\"6cells\")\n",
    "all_results[\"Loss\"].append(\"CrossEntropyLoss\")\n",
    "all_results[\"Optimizer\"].append(\"Adam\")\n",
    "all_results[\"lr\"].append(learning_rate)\n",
    "all_results[\"Accuracy\"].append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map between letters and braille alphabet\n",
    "# 1 is a black dot\n",
    "# 0 is a white dot \n",
    "braille_map = {\"a\":\"100000\", \"b\":\"110000\", \"c\":\"100100\", \"d\":\"100110\", \"e\":\"100010\",\n",
    "               \"f\":\"110100\", \"g\":\"110110\", \"h\":\"110010\", \"i\":\"010100\", \"j\":\"101000\",\n",
    "               \"k\":\"111000\", \"l\":\"101100\", \"m\":\"101110\", \"n\":\"101110\", \"o\":\"101010\",\n",
    "               \"p\":\"111100\", \"q\":\"111110\", \"r\":\"111010\", \"s\":\"011100\", \"t\":\"011110\",\n",
    "               \"u\":\"101001\", \"v\":\"111001\", \"w\":\"010111\", \"x\":\"101101\", \"y\":\"101111\",\n",
    "               \"z\":\"101011\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reverted map to go from dots to letters\n",
    "braille_map_rev = {v:k for k, v in braille_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN classifier module.\n",
    "\n",
    "class CNNSimple6CellsMap(nn.Module):\n",
    "    def __init__(self, num_channels1=16, num_channels2=32):\n",
    "        \"\"\"Models a classifier using the bit strings\"\"\"\n",
    "        super(CNNSimple6CellsMap, self).__init__()\n",
    "        \n",
    "        # Create two sequential layers with convolution, ReLU, BatchNorm and Max Pooling\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(3, num_channels1, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm2d(16),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        self.l2 = nn.Sequential(nn.Conv2d(num_channels1, num_channels2, kernel_size=5, padding=2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm2d(32),\n",
    "                           nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        size_vec = 1568\n",
    "        \n",
    "        # Create linear and sigmoid functions\n",
    "        self.res = nn.Linear(size_vec, 6)\n",
    "        self.finact = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Move from the input layers to the output layer\"\"\"\n",
    "        x = self.l2(self.l1(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = self.finact(self.res(x))\n",
    "        if self.training:\n",
    "            return out\n",
    "        else:\n",
    "            res = []\n",
    "            for t in out:\n",
    "                key = \"\".join(map(lambda a: str(a.item()), torch.round(t).long()))\n",
    "                if key not in braille_map_rev:\n",
    "                    res.append(0)\n",
    "                    continue\n",
    "                res.append(dataset.letter_to_number(braille_map_rev[key]))\n",
    "            res = nn.functional.one_hot(torch.tensor(res), num_classes=26)\n",
    "            return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "num_channels1 = 16\n",
    "num_channels2 = 32\n",
    "num_classes = 26\n",
    "# Create the model\n",
    "model = CNNSimple6CellsMap(num_channels1, num_channels2)\n",
    "\n",
    "# Optimizer\n",
    "num_epochs = 40\n",
    "loss_fn = nn.BCELoss()\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training\n",
    "model, loss_total = train_model(model, train_dataloader, val_dataloader, num_epochs, loss_fn, learning_rate, device='cpu', verbose=True, model_name=\"6cells_map\", optim='SGD', cells=True)\n",
    "\n",
    "# Save the model and display the loss over epochs\n",
    "torch.save(model.state_dict(), 'models/model_6_cells_map.pt')\n",
    "\n",
    "# Plot the results and save the figure\n",
    "plt.title(\"Loss for the 6cells + map model\")\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"BCELoss\")\n",
    "\n",
    "plt.plot(loss_total)\n",
    "plt.savefig(\"plots/model_simple_6cells_map_loss.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bit model\n",
    "model = CNNSimple6CellsMap(num_channels1, num_channels2)\n",
    "# Load the parameters\n",
    "model.load_state_dict(torch.load('models/model_6cells_map_best.pt'))\n",
    "# Compute the accuracy\n",
    "res = eval_model(model, test_dataloader, device='cpu')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill dataframe with the results of this model\n",
    "all_results[\"Model\"].append(\"6cells_map\")\n",
    "all_results[\"Loss\"].append(\"BCELoss\")\n",
    "all_results[\"Optimizer\"].append(\"SGD\")\n",
    "all_results[\"lr\"].append(learning_rate)\n",
    "all_results[\"Accuracy\"].append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"evaluation_results.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
